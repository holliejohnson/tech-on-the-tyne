{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tweets\n",
    "PATH = 'elon_musk_tweets.csv'\n",
    "n_tweets = 50\n",
    "tweets_df = pd.DataFrame(pd.read_csv(PATH)['text'].sample(frac=1)[:n_tweets]) # shuffle and select n tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roberta model setup\n",
    "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess and encode tweets\n",
    "preprocessed_tweets = []\n",
    "encoded_tweets = []\n",
    "\n",
    "for tweet in tweets_df['text']:\n",
    "    preprocessed = (preprocess(tweet))\n",
    "    encoded = tokenizer(preprocessed, return_tensors='pt')\n",
    "    preprocessed_tweets.append(preprocessed)\n",
    "    encoded_tweets.append(encoded)\n",
    "\n",
    "tweets_df['preprocessed'] = preprocessed_tweets\n",
    "tweets_df['encoded'] = encoded_tweets\n",
    "tweets_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_analysis = []\n",
    "for item in tweets_df.encoded:\n",
    "\n",
    "    output = model(**item)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    tweets_analysis.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.concat([tweets_df, pd.DataFrame(tweets_analysis)], axis = 1)\n",
    "tweets_df = tweets_df.rename(columns={0: config.id2label[0], 1: config.id2label[1],2: config.id2label[2]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['sentiment'] = tweets_df[['negative','positive', 'neutral']].idxmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data in a dataframe\n",
    "pd.set_option('max_colwidth', None)\n",
    "pd.set_option('display.width', 3000)\n",
    " \n",
    "# Show a tweet for each sentiment\n",
    "display(tweets_df[tweets_df[\"sentiment\"] == 'positive'].head(1))\n",
    "display(tweets_df[tweets_df[\"sentiment\"] == 'neutral'].head(1))\n",
    "display(tweets_df[tweets_df[\"sentiment\"] == 'negative'].head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_counts = tweets_df.groupby(['sentiment']).size()\n",
    "print(sentiment_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6), dpi=100)\n",
    "ax = plt.subplot(111)\n",
    "sentiment_counts.plot.pie(ax=ax, autopct='%1.1f%%', startangle=270, fontsize=12, label=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud with positive tweets\n",
    "positive_tweets = tweets_df['text'][tweets_df[\"sentiment\"] == 'positive']\n",
    "stop_words = [\"https\", \"co\", \"RT\"] + list(STOPWORDS)\n",
    "positive_wordcloud = WordCloud(max_font_size=50, max_words=50, background_color=\"white\", stopwords = stop_words).generate(str(positive_tweets))\n",
    "plt.figure()\n",
    "plt.title(\"Positive Tweets - Wordcloud\")\n",
    "plt.imshow(positive_wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud with negative tweets\n",
    "negative_tweets = tweets_df['text'][tweets_df[\"sentiment\"] == 'negative']\n",
    "stop_words = [\"https\", \"co\", \"RT\"] + list(STOPWORDS)\n",
    "negative_wordcloud = WordCloud(max_font_size=50, max_words=50, background_color=\"white\", stopwords = stop_words).generate(str(negative_tweets))\n",
    "plt.figure()\n",
    "plt.title(\"Negative Tweets - Wordcloud\")\n",
    "plt.imshow(negative_wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74e486e333988c5da79aa583b454ee8d3e2ddbfa175c0d19197752a76adfe92a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
