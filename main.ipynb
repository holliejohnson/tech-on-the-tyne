{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import tweepy\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter api auth\n",
    "bearer_token = None\n",
    "\n",
    "client = tweepy.Client(bearer_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '(elon musk) lang:en -is:retweet'\n",
    "\n",
    "response = client.search_recent_tweets(query = query, max_results=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#file = open('elon-musk-tweets', 'wb')\n",
    "#pickle.dump(response, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for tweet in response.data:\n",
    "    text = tweet.text\n",
    "    line = {'text' : text}\n",
    "    output.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "twitter_handle = r'@[A-Za-z0-9_]+'                         # remove twitter handle (@username)\n",
    "url_handle = r'http[^ ]+'                                  # remove website URLs that start with 'https?://'\n",
    "combined_handle = r'|'.join((twitter_handle, url_handle))  # join\n",
    "www_handle = r'www.[^ ]+'                                  # remove website URLs that start with 'www.'\n",
    "punctuation_handle = r'\\W+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    souped = soup.get_text()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    try:\n",
    "        text = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        text = souped\n",
    "\n",
    "    cleaned_text = re.sub(punctuation_handle, \" \",(re.sub(www_handle, '', re.sub(combined_handle, '', text)).lower()))\n",
    "    cleaned_text = ' '.join([word for word in cleaned_text.split() if word not in stop_words])\n",
    "\n",
    "    return (\" \".join([word for word in tokenizer.tokenize(cleaned_text) if len(word) > 1])).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets = []\n",
    "\n",
    "for tweet in response.data:\n",
    "    cleaned_tweets.append(process_text(tweet.text))\n",
    "\n",
    "clean_text = pd.DataFrame({'clean_text' : cleaned_tweets})\n",
    "tweets_df = pd.concat([tweets_df, clean_text], axis = 1)\n",
    "\n",
    "#data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "hf_token = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"https://api-inference.huggingface.co/models/\" + model\n",
    "headers = {\"Authorization\": \"Bearer %s\" % (hf_token)}\n",
    "\n",
    "def analysis(data):\n",
    "    payload = dict(inputs=data, options=dict(wait_for_model=True))\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will take some time to run, usually about 60 seconds\n",
    "\n",
    "tweets_analysis = []\n",
    "for tweet in tweets_df.clean_text:\n",
    "    try:\n",
    "        sentiment_result = analysis(tweet)[0]\n",
    "        top_sentiment = max(sentiment_result, key=lambda x: x['score']) # Get the sentiment with the higher score\n",
    "        tweets_analysis.append({'sentiment': top_sentiment['label']})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.concat([tweets_df, pd.DataFrame(tweets_analysis)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data in a dataframe\n",
    "pd.set_option('max_colwidth', None)\n",
    "pd.set_option('display.width', 3000)\n",
    " \n",
    "# Show a tweet for each sentiment\n",
    "display(tweets_df[tweets_df[\"sentiment\"] == 'positive'].head(1))\n",
    "display(tweets_df[tweets_df[\"sentiment\"] == 'neutral'].head(1))\n",
    "display(tweets_df[tweets_df[\"sentiment\"] == 'negative'].head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_counts = tweets_df.groupby(['sentiment']).size()\n",
    "print(sentiment_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6), dpi=100)\n",
    "ax = plt.subplot(111)\n",
    "sentiment_counts.plot.pie(ax=ax, autopct='%1.1f%%', startangle=270, fontsize=12, label=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud with positive tweets\n",
    "positive_tweets = tweets_df['clean_text'][tweets_df[\"sentiment\"] == 'positive']\n",
    "stop_words = [\"https\", \"co\", \"RT\"] + list(STOPWORDS)\n",
    "positive_wordcloud = WordCloud(max_font_size=50, max_words=50, background_color=\"white\", stopwords = stop_words).generate(str(positive_tweets))\n",
    "plt.figure()\n",
    "plt.title(\"Positive Tweets - Wordcloud\")\n",
    "plt.imshow(positive_wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    " \n",
    "# Wordcloud with negative tweets\n",
    "negative_tweets = tweets_df['clean_text'][tweets_df[\"sentiment\"] == 'negative']\n",
    "stop_words = [\"https\", \"co\", \"RT\"] + list(STOPWORDS)\n",
    "negative_wordcloud = WordCloud(max_font_size=50, max_words=50, background_color=\"white\", stopwords = stop_words).generate(str(negative_tweets))\n",
    "plt.figure()\n",
    "plt.title(\"Negative Tweets - Wordcloud\")\n",
    "plt.imshow(negative_wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74e486e333988c5da79aa583b454ee8d3e2ddbfa175c0d19197752a76adfe92a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
